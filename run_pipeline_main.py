# -*- coding: utf-8 -*-
"""i220592_AbduHaadi_A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OnMgKq1vXo2SDV9R_FsTg7T_xMRseNUh

# **Name: Abdul Haadi Bin Irfan**
# **Roll No: I22-0592**
# **Section: AI-A**

# **Assignment 2:**

# **Q#1: (100 marks)**
**Perform the following tasks for this question.**

# **Part A – (25 marks)**
**1. Build a TFX pipeline for the Penguin dataset, including at a minimum:**

**o ExampleGen**

**o StatisticsGen**

**o SchemaGen**

**o ExampleValidator**

**o Transform**

**o Trainer (using TensorFlow Estimator or Keras)**

**o Evaluator**

**o Pusher**

**2. Modify the pipeline to:**

**o Store metadata in ML Metadata (MLMD).**

**o Ensure transformations (e.g., normalization, vocab generation) are consistent at training and serving.**

**o Produce evaluation metrics sliced by island (feature).****
"""

# Colab-friendly TFX-like pipeline for Palmer Penguins (no TFX dependency).
# Paste this entire cell into Colab and run.

# If seaborn not installed, uncomment the next line:
# !pip install seaborn

import os
import sqlite3
import json
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import seaborn as sns
from sklearn.model_selection import train_test_split
from datetime import datetime, timezone


# -------------------------
# Config / Paths
# -------------------------
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
SERVING_MODEL_DIR = "serving_model"
os.makedirs(SERVING_MODEL_DIR, exist_ok=True)
METADATA_DB = "pipeline_metadata.db"

# -------------------------
# ExampleGen: load CSV (use seaborn penguins)
# -------------------------
print("ExampleGen: loading dataset")
df = sns.load_dataset("penguins")  # uses palmerpenguins built in seaborn
# preview
print(df.head())

# Standardize column names for pipeline
# We expect: bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, island, sex, species
df = df.rename(columns=lambda c: c.strip())
expected_cols = ['species','island','bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g','sex']
df = df[expected_cols].copy()

# -------------------------
# StatisticsGen: basic statistics
# -------------------------
print("\nStatisticsGen: computing stats")
stats = df.describe(include='all').to_dict()
print("Numerical stats (example):", {k: stats[k] for k in stats if 'bill_length_mm' in k or 'body_mass_g' in k} if isinstance(stats, dict) else stats)

# Save stats JSON for record
with open(os.path.join(DATA_DIR, "statistics.json"), "w") as f:
    json.dump(stats, f, default=str)

# -------------------------
# SchemaGen: infer types & allowed missing
# -------------------------
print("\nSchemaGen: inferring schema")
schema = {}
for col in df.columns:
    dtype = str(df[col].dtype)
    n_unique = int(df[col].nunique(dropna=True))
    n_null = int(df[col].isna().sum())
    schema[col] = {"dtype": dtype, "n_unique": n_unique, "n_null": n_null}
print(schema)
with open(os.path.join(DATA_DIR, "schema.json"), "w") as f:
    json.dump(schema, f)

# -------------------------
# ExampleValidator: simple checks
# -------------------------
print("\nExampleValidator: validating examples")
issues = []
# Check required columns present
for c in expected_cols:
    if c not in df.columns:
        issues.append(f"Missing column: {c}")
# Check too many missing values
for c in df.columns:
    pct_missing = df[c].isna().mean()
    if pct_missing > 0.3:
        issues.append(f"Column {c} has >30% missing ({pct_missing:.2f})")
# Report
if issues:
    print("Validation issues found:")
    for it in issues:
        print(" -", it)
else:
    print("No validation issues found (or only tolerable missing).")

# For simplicity, drop rows with missing label or island. For features, we'll fill later.
df = df.dropna(subset=["species", "island"]).reset_index(drop=True)
print(f"Rows after dropping missing label/island: {len(df)}")

# -------------------------
# Transform: build preprocessing layers (Normalization + StringLookup)
# -------------------------
print("\nTransform: building preprocessing layers and preparing train/test splits")

# Split dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['species'])
train_df = train_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

# Features and label
NUMERIC_FEATURES = ['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']
CATEGORICAL_FEATURES = ['island','sex']
LABEL_KEY = 'species'

# Fill numeric missing with median, categorical missing with 'NA'
for col in NUMERIC_FEATURES:
    median_val = train_df[col].median()
    train_df[col] = train_df[col].fillna(median_val)
    test_df[col] = test_df[col].fillna(median_val)

for col in CATEGORICAL_FEATURES:
    train_df[col] = train_df[col].fillna("NA")
    test_df[col] = test_df[col].fillna("NA")

# Build preprocessing layers and adapt to training data
normalizers = {}
for col in NUMERIC_FEATURES:
    layer = tf.keras.layers.Normalization(axis=None, name=f"norm_{col}")
    # adapt expects 1D array
    layer.adapt(train_df[col].astype(np.float32).to_numpy().reshape(-1,1))
    normalizers[col] = layer

# categorical StringLookup
string_lookups = {}
for col in CATEGORICAL_FEATURES:
    sl = tf.keras.layers.StringLookup(output_mode='int', oov_token="[UNK]", name=f"lookup_{col}")
    sl.adapt(train_df[col].astype(str).to_numpy())
    string_lookups[col] = sl

# Build a Keras preprocessing model that takes raw inputs and outputs processed tensors
def build_preprocessing_model(train_df):
    # Numeric features
    numeric_features = ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]
    numeric_inputs = {col: layers.Input(shape=(1,), name=col) for col in numeric_features}
    normalizers = {}
    for col in numeric_features:
        norm = layers.Normalization(axis=None)
        norm.adapt(train_df[col].dropna().values.reshape(-1, 1))  # fit scaler
        normalizers[col] = norm

    # Categorical features
    categorical_features = ["island", "sex"]
    categorical_inputs = {col: layers.Input(shape=(1,), dtype=tf.string, name=col) for col in categorical_features}
    lookups = {}
    for col in categorical_features:
        lookup = layers.StringLookup(output_mode="one_hot")
        lookup.adapt(train_df[col].dropna().values)  # fit vocab
        lookups[col] = lookup

    # Apply transforms
    norm_outs = [normalizers[col](numeric_inputs[col]) for col in numeric_features]
    cat_outs = [lookups[col](categorical_inputs[col]) for col in categorical_features]

    all_features = layers.concatenate(norm_outs + cat_outs)

    preprocessing_model = keras.Model(
        inputs=[*numeric_inputs.values(), *categorical_inputs.values()],
        outputs=all_features,
        name="preprocessing"
    )
    return preprocessing_model


preprocessing_model = build_preprocessing_model(train_df)

preprocessing_model.summary()

# -------------------------
# Trainer: build model that accepts preprocessed inputs (or raw via composed model later)
# -------------------------
print("\nTrainer: building and training model")

# Determine label classes and map to ints
label_vocab = train_df[LABEL_KEY].unique().tolist()
label2idx = {v:i for i,v in enumerate(sorted(label_vocab))}
num_classes = len(label2idx)
print("Label mapping:", label2idx)

# -------------------------
# Trainer: end-to-end model with preprocessing inside
# -------------------------

def make_full_model():
    # Raw inputs (numerical + categorical)
    raw_inputs = {}
    for col in NUMERIC_FEATURES:
        raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.float32)
    for col in CATEGORICAL_FEATURES:
        raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.string)

    # Pass raw inputs through preprocessing model
    x = preprocessing_model(raw_inputs)

    # Dense layers (trainable)
    x = keras.layers.Dense(64, activation="relu")(x)
    x = keras.layers.Dense(32, activation="relu")(x)
    outputs = keras.layers.Dense(num_classes, activation="softmax", name="predictions")(x)

    # Full model
    model = keras.Model(inputs=raw_inputs, outputs=outputs, name="penguin_model")
    model.compile(optimizer="adam",
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])
    return model

# Build model
model = make_full_model()
model.summary()



# Prepare tf.data datasets (no preprocessing step required now)
def df_to_dataset(df_in, batch_size=32, shuffle=True):
    inputs = {}
    for col in NUMERIC_FEATURES:
        inputs[col] = df_in[col].astype(np.float32).values.reshape(-1,1)
    for col in CATEGORICAL_FEATURES:
        inputs[col] = df_in[col].astype(str).values.reshape(-1,1)
    labels = np.array([label2idx[s] for s in df_in[LABEL_KEY].astype(str)])
    ds = tf.data.Dataset.from_tensor_slices((inputs, labels))
    if shuffle:
        ds = ds.shuffle(len(df_in))
    return ds.batch(batch_size)


# Train/valid datasets
train_ds = df_to_dataset(train_df, batch_size=32, shuffle=True)
valid_ds = df_to_dataset(test_df, batch_size=32, shuffle=False)

# Train
EPOCHS = 15
history = model.fit(train_ds, validation_data=valid_ds, epochs=EPOCHS)

# -------------------------
# Pusher: exporting serving model
# -------------------------
print("\nPusher: exporting serving model that accepts raw features and applies same preprocessing")

# Raw inputs
raw_inputs = {}
for col in NUMERIC_FEATURES:
    raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.float32)
for col in CATEGORICAL_FEATURES:
    raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.string)

# Apply preprocessing model
processed = preprocessing_model(raw_inputs)

# Reuse trained dense layers from classifier
x = model.layers[-3](processed)   # dense_12
x = model.layers[-2](x)           # dense_13
outputs = model.layers[-1](x)     # predictions

# Build serving model
serving_model = keras.Model(inputs=raw_inputs, outputs=outputs, name="serving_model")

# Save (Keras 3 style)

timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
export_path = os.path.join(SERVING_MODEL_DIR, f"penguin_model_{timestamp}")
os.makedirs(export_path, exist_ok=True)

# TF SavedModel format
serving_model.export(export_path)

print("Serving model saved to:", export_path)


# -------------------------
# Evaluator: overall and sliced-by-island accuracy
# -------------------------
print("\nEvaluator: computing overall and per-island metrics")

# Ensure categorical and label columns are proper string dtype with no NaNs
for col in ["island", "sex", LABEL_KEY]:
    test_df[col] = test_df[col].fillna("Unknown").astype(str)

# Helper to get predicted labels for a raw dataframe via serving_model
def predict_raw_df(df_in):
    import tensorflow as tf

    input_dict = {}
    # numeric features: float32
    for col in NUMERIC_FEATURES:
        input_dict[col] = tf.constant(
            df_in[col].astype(np.float32).to_numpy().reshape(-1, 1),
            dtype=tf.float32
        )

    # categorical features: tf.string
    for col in CATEGORICAL_FEATURES:
        input_dict[col] = tf.constant(
            df_in[col].fillna("Unknown").astype(str).to_numpy().reshape(-1, 1),
            dtype=tf.string
        )

    preds = serving_model.predict(input_dict, batch_size=32, verbose=0)
    pred_idx = np.argmax(preds, axis=1)
    pred_labels = [sorted(label_vocab)[i] for i in pred_idx]
    return pred_labels, pred_idx





# overall
pred_labels_test, pred_idx_test = predict_raw_df(test_df)
true_labels_test = test_df[LABEL_KEY].tolist()
overall_acc = np.mean([p==t for p,t in zip(pred_labels_test, true_labels_test)])
print(f"Overall accuracy on test set: {overall_acc:.4f}")



# per-island slicing
slice_results = {}
for island_name, group in test_df.groupby("island"):
    preds, _ = predict_raw_df(group)
    trues = group[LABEL_KEY].astype(str).tolist()
    acc = np.mean([p==t for p,t in zip(preds,trues)])
    slice_results[island_name] = {"n": len(group), "accuracy": float(acc)}
print("Per-island slice metrics:", slice_results)

# Save evaluation report
eval_report = {"overall_accuracy": float(overall_acc), "slices": slice_results}
with open(os.path.join(DATA_DIR, "evaluation_report.json"), "w") as f:
    json.dump(eval_report, f, indent=2)

# -------------------------
# Metadata (MLMD-like): store run info in sqlite
# -------------------------
print("\nStoring pipeline metadata to sqlite (simple MLMD-like store)")

conn = sqlite3.connect(METADATA_DB)
cur = conn.cursor()
# Create tables (if not exist)
cur.execute("""
CREATE TABLE IF NOT EXISTS runs (
    run_id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_time TEXT,
    dataset_path TEXT,
    model_path TEXT,
    overall_accuracy REAL,
    extra JSON
)
""")
conn.commit()

extra = {"schema": schema, "stats_sample": {k:v for k,v in list(stats.items())[:3]}}
cur.execute("INSERT INTO runs (run_time, dataset_path, model_path, overall_accuracy, extra) VALUES (?, ?, ?, ?, ?)",
            (datetime.utcnow().isoformat(), "seaborn.penguins", export_path, float(overall_acc), json.dumps(extra)))
conn.commit()
conn.close()
print("Metadata saved to", METADATA_DB)

# -------------------------
# Done. Summary
# -------------------------
print("\nPIPELINE COMPLETE.")
print(f"- Model exported to: {export_path}")
print(f"- Evaluation report: {os.path.join(DATA_DIR, 'evaluation_report.json')}")
print(f"- Metadata DB: {METADATA_DB}")



"""# **Part B – (25 marks)**

**1. Inject anomalies into the dataset (e.g., missing values, wrong types, out-of-range features). Show how TFX components detect and block bad data.**

**2. Simulate data drift by adding a shifted distribution (e.g., altering flipper length). Show
how Evaluator + Schema evolution handles this.**

**3. Extend SchemaGen so that new feature values are either:
o Marked as schema updates, or
o Blocked from entering the model.**

**Step 1: Inject anomalies into the dataset**

**We’ll create a “bad batch” with:**

**1. Missing values**

**2. Wrong types (string in numeric columns)**

**3. Out-of-range values**
"""

import copy
import numpy as np

# Make a copy of test_df to inject anomalies
bad_df = copy.deepcopy(test_df)

# 1. Inject missing values
bad_df.loc[0, "bill_length_mm"] = np.nan
bad_df.loc[1, "sex"] = None

# 2. Wrong types: cast column to object first
bad_df["body_mass_g"] = bad_df["body_mass_g"].astype(object)
bad_df.loc[2, "body_mass_g"] = "heavy"  # string instead of float

# 3. Out-of-range values
bad_df.loc[3, "flipper_length_mm"] = -50  # negative flipper length

print(bad_df.head(5))

"""**Step 2: Run ExampleValidator on the anomalous batch**

**Modify your ExampleValidator logic to detect bad data:**
"""

print("\nExampleValidator: detecting anomalies in bad_df")
issues = []

# Check required columns present
for c in expected_cols:
    if c not in bad_df.columns:
        issues.append(f"Missing column: {c}")

# Check missing values, type issues, out-of-range
for c in NUMERIC_FEATURES:
    # Missing
    missing_idx = bad_df[c].isna()
    if missing_idx.any():
        issues.append(f"Column {c} has missing values at rows {list(bad_df[missing_idx].index)}")

    # Wrong type
    wrong_type_idx = bad_df[c].apply(lambda x: not isinstance(x, (int, float, np.integer, np.floating)))
    if wrong_type_idx.any():
        issues.append(f"Column {c} has wrong type at rows {list(bad_df[wrong_type_idx].index)}")

    # Out-of-range check (only numeric values)
    numeric_idx = bad_df[c].apply(lambda x: isinstance(x, (int, float, np.integer, np.floating)))
    # Only apply "< 0" to numeric values
    out_of_range_idx = bad_df.index[numeric_idx & (bad_df.loc[numeric_idx, c] < 0)]
    if len(out_of_range_idx) > 0:
        issues.append(f"Column {c} has out-of-range values at rows {list(out_of_range_idx)}")

# Categorical columns: missing values
for c in CATEGORICAL_FEATURES:
    missing_idx = bad_df[c].isna()
    if missing_idx.any():
        issues.append(f"Column {c} has missing values at rows {list(bad_df[missing_idx].index)}")

# Report
if issues:
    print("Validation issues found in bad_df:")
    for it in issues:
        print(" -", it)
else:
    print("No validation issues found (bad_df passed checks)")

"""**Step 3: Simulate data drift**

**For example, shift flipper_length_mm by +10% in a new batch:**
"""

drift_df = copy.deepcopy(test_df)
drift_df["flipper_length_mm"] = drift_df["flipper_length_mm"] * 1.1  # +10% shift

# Compare statistics to training batch
train_mean = train_df["flipper_length_mm"].mean()
drift_mean = drift_df["flipper_length_mm"].mean()
print(f"Training mean flipper_length_mm: {train_mean:.2f}")
print(f"Drifted batch mean flipper_length_mm: {drift_mean:.2f}")
print("Difference:", drift_mean - train_mean)

if abs(drift_mean - train_mean) / train_mean > 0.05:  # >5% shift
    print("⚠️ Data drift detected in flipper_length_mm")

"""**Step 4: Extend SchemaGen for schema evolution**

**Current SchemaGen stores allowed types and unique values. To handle new unseen values:**
"""

# Suppose new island appears
new_value_df = pd.DataFrame({"island":["Torgersen", "Dream"],
                             "bill_length_mm":[40, 50],
                             "bill_depth_mm":[18, 17],
                             "flipper_length_mm":[200, 210],
                             "body_mass_g":[3500, 4000],
                             "sex":["MALE","FEMALE"],
                             "species":["Adelie","Chinstrap"]})

# Check against schema
for col in CATEGORICAL_FEATURES:
    allowed_values = train_df[col].unique().tolist()
    new_vals = set(new_value_df[col]) - set(allowed_values)
    if new_vals:
        print(f"Schema alert: column '{col}' has new values {new_vals}")
        # Decide action: block or update schema
        # For now, we block:
        new_value_df = new_value_df[~new_value_df[col].isin(new_vals)]

print("After schema enforcement, rows allowed into model:", len(new_value_df))



"""# **Part C – (20 marks)**
**1. Train two models:**

**Model A: Baseline (default hyperparameters)**
"""

# Baseline model (same as your previous model)
def make_model_a():
    raw_inputs = {}
    for col in NUMERIC_FEATURES:
        raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.float32)
    for col in CATEGORICAL_FEATURES:
        raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.string)

    x = preprocessing_model(raw_inputs)  # reuse preprocessing

    # Baseline dense layers
    x = keras.layers.Dense(64, activation="relu")(x)
    x = keras.layers.Dense(32, activation="relu")(x)
    outputs = keras.layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs=raw_inputs, outputs=outputs, name="model_a_baseline")
    model.compile(optimizer="adam",
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])
    return model

model_a = make_model_a()
model_a.summary()

# Train
history_a = model_a.fit(train_ds, validation_data=valid_ds, epochs=15)



"""**Model B: Tuned (change architecture, optimizer, learning rate, or
regularization)**
"""

# Tuned model: changed architecture, optimizer, learning rate
def make_model_b():
    raw_inputs = {}
    for col in NUMERIC_FEATURES:
        raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.float32)
    for col in CATEGORICAL_FEATURES:
        raw_inputs[col] = keras.Input(shape=(1,), name=col, dtype=tf.string)

    x = preprocessing_model(raw_inputs)

    # Tuned dense layers: more neurons + dropout
    x = keras.layers.Dense(128, activation="relu", kernel_regularizer=keras.regularizers.l2(0.001))(x)
    x = keras.layers.Dropout(0.3)(x)
    x = keras.layers.Dense(64, activation="relu", kernel_regularizer=keras.regularizers.l2(0.001))(x)
    outputs = keras.layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs=raw_inputs, outputs=outputs, name="model_b_tuned")
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])
    return model

model_b = make_model_b()
model_b.summary()

# Train
history_b = model_b.fit(train_ds, validation_data=valid_ds, epochs=15)

"""**2. Use Evaluator to:**

**o Compare both models.**

**o Define thresholds for accuracy & AUC.**

**o Block any model that doesn’t surpass the baseline.**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import LabelBinarizer

# Step 0: Prepare evaluation dataframe
eval_df = test_df.copy()

# 1️⃣ Ensure numeric columns are float32 & fill missing values
for col in NUMERIC_FEATURES:
    eval_df[col] = pd.to_numeric(eval_df[col], errors='coerce')
    median_val = train_df[col].median()
    eval_df[col] = eval_df[col].fillna(median_val).astype(np.float32)

# 2️⃣ Ensure categorical columns are strings
for col in CATEGORICAL_FEATURES:
    eval_df[col] = eval_df[col].astype(str)

# Step 1: Helper function to get predictions
def get_preds(model, df_in):
    inputs = {}
    for col in NUMERIC_FEATURES:
        inputs[col] = tf.convert_to_tensor(df_in[col].values.reshape(-1,1), dtype=tf.float32)
    for col in CATEGORICAL_FEATURES:
        inputs[col] = tf.convert_to_tensor(df_in[col].values.reshape(-1,1), dtype=tf.string)
    preds = model.predict(inputs, batch_size=32, verbose=0)
    pred_labels = np.argmax(preds, axis=1)
    return pred_labels, preds

# Step 2: True labels
y_true = np.array([label2idx[s] for s in test_df[LABEL_KEY]])

# Step 3: Metrics for Model A (Baseline)
pred_a, prob_a = get_preds(model_a, eval_df)
acc_a = accuracy_score(y_true, pred_a)

# Step 4: One-hot encode labels for AUC
lb = LabelBinarizer()
lb.fit(y_true)
y_true_bin = lb.transform(y_true)
auc_a = roc_auc_score(y_true_bin, prob_a, multi_class='ovr')

# Step 5: Metrics for Model B (Tuned)
pred_b, prob_b = get_preds(model_b, eval_df)
acc_b = accuracy_score(y_true, pred_b)
auc_b = roc_auc_score(y_true_bin, prob_b, multi_class='ovr')

# Step 6: Display metrics table
metrics_df = pd.DataFrame({
    "Model": ["Baseline (A)", "Tuned (B)"],
    "Accuracy": [acc_a, acc_b],
    "AUC": [auc_a, auc_b]
})
print(metrics_df)

# Step 7: Threshold enforcement (baseline as reference)
ACC_THRESHOLD = acc_a
AUC_THRESHOLD = auc_a

def check_thresholds(model_name, acc, auc):
    if acc < ACC_THRESHOLD or auc < AUC_THRESHOLD:
        print(f"{model_name} blocked: does not surpass baseline thresholds")
    else:
        print(f"{model_name} passes baseline thresholds")

check_thresholds("Baseline (A)", acc_a, auc_a)
check_thresholds("Tuned (B)", acc_b, auc_b)

"""# **Comparative Analysis:**

**1. Metrics Comparison:**


*   **Both Model A (Baseline) and Model B (Tuned) achieved Accuracy = 1.0 and AUC = 1.0 on the test set.**
*   **This indicates that, on this dataset, the tuned changes (architecture, optimizer, etc.) did not improve or reduce performance compared to the baseline.**


**2. Threshold Enforcement:**



*   **The Evaluator used the baseline thresholds (Accuracy ≥ 1.0, AUC ≥ 1.0).**
*   **Both models pass the thresholds, so no model is blocked.**


**3. Conclusion:**


*   **Since both models achieve perfect metrics, the baseline thresholds do not block any model.**
*   **If the tuned model had underperformed, the Evaluator would have automatically blocked it.**







"""



"""# **Part D – (20 marks)**

**Step 1: Modular Pipeline in Python**
"""

import os
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import LabelBinarizer, LabelEncoder

# -----------------------------
# 1️⃣ Data Validation & Schema Enforcement
# -----------------------------
def validate_and_enforce_schema(df, train_df, numeric_features, categorical_features):
    df_clean = df.copy()

    # Numeric columns: force float32 & fill missing
    for col in numeric_features:
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        df_clean[col] = df_clean[col].fillna(train_df[col].median()).astype(np.float32)

    # Categorical columns: keep only seen categories
    for col in categorical_features:
        allowed_values = train_df[col].astype(str).unique().tolist()
        df_clean[col] = df_clean[col].astype(str)
        df_clean = df_clean[df_clean[col].isin(allowed_values)]

    return df_clean

# -----------------------------
# 2️⃣ Encode categorical columns
# -----------------------------
def encode_categorical(df, train_df, categorical_features):
    encoders = {}
    df_encoded = df.copy()
    for col in categorical_features:
        le = LabelEncoder()
        le.fit(train_df[col].astype(str))
        df_encoded[col] = le.transform(df_encoded[col].astype(str))
        encoders[col] = le
    return df_encoded, encoders

# -----------------------------
# 3️⃣ Train Model
# -----------------------------
def train_model(train_df, numeric_features, categorical_features, label_key, model_params):
    from tensorflow.keras import Input, Model
    from tensorflow.keras.layers import Dense, Concatenate

    # Prepare inputs
    inputs = []
    concat_layers = []
    for col in numeric_features + categorical_features:
        inp = Input(shape=(1,), name=col)
        inputs.append(inp)
        concat_layers.append(inp)

    x = Concatenate()(concat_layers)
    x = Dense(model_params.get("hidden_units", 16), activation='relu')(x)
    output = Dense(model_params.get("num_classes", 3), activation='softmax')(x)

    model = Model(inputs=inputs, outputs=output)
    model.compile(optimizer=model_params.get("optimizer", "adam"),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    # Prepare data dict
    X_train = {col: train_df[col].values.reshape(-1,1) for col in numeric_features + categorical_features}
    y_train = train_df[label_key].values

    model.fit(X_train, y_train, epochs=model_params.get("epochs",5), batch_size=32, verbose=0)
    return model

# -----------------------------
# 4️⃣ Evaluate Model
# -----------------------------
def evaluate_model(model, df, numeric_features, categorical_features, label_key):
    X_eval = {col: df[col].values.reshape(-1,1) for col in numeric_features + categorical_features}
    y_true = df[label_key].values
    preds = model.predict(X_eval, batch_size=32, verbose=0)
    pred_labels = np.argmax(preds, axis=1)

    acc = accuracy_score(y_true, pred_labels)

    lb = LabelBinarizer()
    lb.fit(y_true)
    y_true_bin = lb.transform(y_true)
    auc = roc_auc_score(y_true_bin, preds, multi_class='ovr')

    return acc, auc

# -----------------------------
# 5️⃣ Deploy Model
# -----------------------------
def deploy_model(model, serving_dir, model_name="final_model.h5"):
    if not os.path.exists(serving_dir):
        os.makedirs(serving_dir)
    path = os.path.join(serving_dir, model_name)
    model.save(path)
    print(f"Model deployed at {path}")

import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, LabelBinarizer
from sklearn.metrics import accuracy_score, roc_auc_score
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Concatenate

# ----- Existing functions: validate_and_enforce_schema, encode_categorical, train_model, evaluate_model, deploy_model -----
# (Keep your previous implementations here)

def run_pipeline(train_df, test_df, numeric_features, categorical_features, label_key, serving_dir):
    # 1️⃣ Encode labels
    le_label = LabelEncoder()
    train_df[label_key] = le_label.fit_transform(train_df[label_key])
    test_df[label_key] = le_label.transform(test_df[label_key])

    # 2️⃣ Validate & encode categorical features
    train_df_encoded, encoders = encode_categorical(train_df, train_df, categorical_features)
    test_df_encoded = validate_and_enforce_schema(test_df, train_df, numeric_features, categorical_features)
    test_df_encoded, _ = encode_categorical(test_df_encoded, train_df, categorical_features)

    # 3️⃣ Train baseline model
    baseline_params = {"hidden_units":16, "epochs":5, "optimizer":"adam", "num_classes":len(le_label.classes_)}
    model_a = train_model(train_df_encoded, numeric_features, categorical_features, label_key, baseline_params)

    # 4️⃣ Train tuned model
    tuned_params = {"hidden_units":32, "epochs":10, "optimizer":"adam", "num_classes":len(le_label.classes_)}
    model_b = train_model(train_df_encoded, numeric_features, categorical_features, label_key, tuned_params)

    # 5️⃣ Evaluate models
    acc_a, auc_a = evaluate_model(model_a, test_df_encoded, numeric_features, categorical_features, label_key)
    acc_b, auc_b = evaluate_model(model_b, test_df_encoded, numeric_features, categorical_features, label_key)

    # 6️⃣ Log metrics to CSV
    metrics_df = pd.DataFrame({
        "Model": ["Baseline (A)", "Tuned (B)"],
        "Accuracy": [acc_a, acc_b],
        "AUC": [auc_a, auc_b]
    })

    if not os.path.exists(serving_dir):
        os.makedirs(serving_dir)
    metrics_path = os.path.join(serving_dir, "metrics.csv")
    metrics_df.to_csv(metrics_path, index=False)
    print(metrics_df)
    print(f"Metrics logged at {metrics_path}")

    # 7️⃣ Deploy only validated models
    ACC_THRESHOLD = acc_a  # baseline thresholds
    AUC_THRESHOLD = auc_a

    if acc_a >= ACC_THRESHOLD and auc_a >= AUC_THRESHOLD:
        deploy_model(model_a, serving_dir, "baseline_model.keras")
    if acc_b >= ACC_THRESHOLD and auc_b >= AUC_THRESHOLD:
        deploy_model(model_b, serving_dir, "tuned_model.keras")

import seaborn as sns
from sklearn.model_selection import train_test_split

# Load data
penguins = sns.load_dataset("penguins").dropna()
train_df, test_df = train_test_split(penguins, test_size=0.2, random_state=42)

NUMERIC_FEATURES = ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]
CATEGORICAL_FEATURES = ["sex", "island"]
LABEL_KEY = "species"
SERVING_DIR = "serving_model_dir"

# Run the pipeline
run_pipeline(train_df, test_df, NUMERIC_FEATURES, CATEGORICAL_FEATURES, LABEL_KEY, SERVING_DIR)


if __name__ == "__main__":
    import seaborn as sns
    from sklearn.model_selection import train_test_split

    # Load data
    penguins = sns.load_dataset("penguins").dropna()
    train_df, test_df = train_test_split(penguins, test_size=0.2, random_state=42)

    NUMERIC_FEATURES = ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]
    CATEGORICAL_FEATURES = ["sex", "island"]
    LABEL_KEY = "species"
    SERVING_DIR = "serving_model_dir"

    # Run the pipeline
    run_pipeline(train_df, test_df, NUMERIC_FEATURES, CATEGORICAL_FEATURES, LABEL_KEY, SERVING_DIR)


